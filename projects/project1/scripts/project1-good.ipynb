{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Methods.costs import * \n",
    "from Methods.least_squares import * \n",
    "from Methods.ridge import *\n",
    "from Methods.cross_validation import *\n",
    "from Methods.split_data import *\n",
    "from Methods.scaling_standardization import *\n",
    "from Methods.build_polynomial import *\n",
    "from Methods.clearDataset import *\n",
    "from Methods.logistic import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Methods.proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'csv/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'csv/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX, tX_test = averageData(tX, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot(tX.T, -1000, 2000, 'boxplot_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX, tX_test = data_scaling(tX.T, tX_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/build_polynomial.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  new = np.array([np.log(x)])\n",
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/build_polynomial.py:38: RuntimeWarning: invalid value encountered in log\n",
      "  new = np.array([np.log(x)])\n",
      "/home/huguenin/anaconda3/lib/python3.5/site-packages/numpy/lib/function_base.py:964: RuntimeWarning: invalid value encountered in multiply\n",
      "  scl = np.multiply(avg, 0) + scl\n",
      "/home/huguenin/anaconda3/lib/python3.5/site-packages/numpy/lib/function_base.py:2490: RuntimeWarning: invalid value encountered in subtract\n",
      "  X -= avg[:, None]\n",
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/build_polynomial.py:25: RuntimeWarning: invalid value encountered in sqrt\n",
      "  new = np.array([np.sqrt(x)])\n"
     ]
    }
   ],
   "source": [
    "#y, x, tX, method,  tX_test, x_test, **kwargs\n",
    "for i in range(0, 30): \n",
    "    tX, tX_test = add_feature(y, tX[i], tX, log_def, tX_test, tX_test[i])\n",
    "    tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=2)\n",
    "    tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=3)\n",
    "    tX, tX_test = add_feature(y, tX[i], tX, sqrt_def, tX_test, tX_test[i])\n",
    "    #tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=4)\n",
    "    #tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=5)\n",
    "    #tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=6)\n",
    "    #tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=7)\n",
    "    #tX, tX_test = add_feature(y, tX[i], tX, multiply, tX_test, tX_test[i], degree=8)   \n",
    "\n",
    "tX = tX.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8072\n",
      "1\n",
      "[0.88436191686435706]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 1\n",
    "k_fold = 10\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "k_list = list(range(k_fold))\n",
    "#k=7\n",
    "tot_loss_tr = 0\n",
    "tot_loss_te = 0\n",
    "best_accuracy = 0\n",
    "best_k = 0\n",
    "weights = np.array([])\n",
    "for k in k_list:\n",
    "    loss_tr, loss_te, accuracy_least, w = cross_validation(y, tX, k_indices, k, least_squares)\n",
    "    tot_loss_tr += loss_tr\n",
    "    tot_loss_te += loss_te\n",
    "    if accuracy_least > best_accuracy:\n",
    "        best_accuracy = accuracy_least\n",
    "        best_k = k\n",
    "        weights = w\n",
    "rmse_tr.append(np.sqrt(2/k_fold * tot_loss_tr))\n",
    "rmse_te.append(np.sqrt(2/k_fold * tot_loss_te))\n",
    "print(best_accuracy)\n",
    "print(best_k)\n",
    "print(rmse_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794448\n",
      "2\n",
      "1e-10\n",
      "0.907908952851\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEdCAYAAAASHSDrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f112bd40898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "k_list = list(range(k_fold))\n",
    "#k=7\n",
    "lambdas = np.logspace(-10, -2, 20)\n",
    "tot_loss_tr = 0\n",
    "tot_loss_te = 0\n",
    "best_accuracy = 0\n",
    "best_k = 0\n",
    "best_l = 0\n",
    "weights = np.array([])\n",
    "for l in lambdas:\n",
    "    for k in k_list:\n",
    "        loss_tr, loss_te, accuracy_least, w = cross_validation(y, tX, k_indices, k, ridge_regression, lambda_=l)\n",
    "        tot_loss_tr += loss_tr\n",
    "        tot_loss_te += loss_te\n",
    "        if accuracy_least > best_accuracy:\n",
    "            best_accuracy = accuracy_least\n",
    "            best_k = k\n",
    "            weights = w\n",
    "            best_l = l\n",
    "    rmse_tr.append(np.sqrt(2/k_fold * tot_loss_tr))\n",
    "    rmse_te.append(np.sqrt(2/k_fold * tot_loss_te))\n",
    "    \n",
    "cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "print(best_accuracy)\n",
    "print(best_k)\n",
    "print(best_l)\n",
    "print(np.min(rmse_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0.6931471805599078\n",
      "Current iteration=10, the loss=0.6723248424993514\n",
      "Current iteration=20, the loss=0.6654559375322797\n",
      "Current iteration=30, the loss=0.6372048468308211\n",
      "Current iteration=40, the loss=0.6345797133966027\n",
      "Current iteration=50, the loss=0.6111565662809639\n",
      "Current iteration=60, the loss=0.5929277661176704\n",
      "Current iteration=70, the loss=0.5839925544016811\n",
      "Current iteration=80, the loss=0.57055684966386\n",
      "Current iteration=90, the loss=0.5625117118518622\n",
      "Current iteration=100, the loss=0.5483011136281181\n",
      "Current iteration=110, the loss=0.5423213803761209\n",
      "Current iteration=120, the loss=0.5322094800134617\n",
      "Current iteration=130, the loss=0.5251348872942521\n",
      "Current iteration=140, the loss=0.5183054026205871\n",
      "Current iteration=150, the loss=0.5168654388271088\n",
      "Current iteration=160, the loss=0.5031282525053772\n",
      "Current iteration=170, the loss=0.4952186368243647\n",
      "Current iteration=180, the loss=0.4836585513775422\n",
      "Current iteration=190, the loss=0.49291995105485253\n",
      "Current iteration=200, the loss=0.49374639561734657\n",
      "Current iteration=210, the loss=0.4733966364493936\n",
      "Current iteration=220, the loss=0.46946824227656264\n",
      "Current iteration=230, the loss=0.47918355074075325\n",
      "Current iteration=240, the loss=0.46423613730291347\n",
      "Current iteration=250, the loss=0.46247449973502336\n",
      "Current iteration=260, the loss=0.45810471582876205\n",
      "Current iteration=270, the loss=0.4471592901860698\n",
      "Current iteration=280, the loss=0.44978267464442\n",
      "Current iteration=290, the loss=0.43931929912722756\n",
      "Current iteration=300, the loss=0.4400547267779072\n",
      "Current iteration=310, the loss=0.43758343863849736\n",
      "Current iteration=320, the loss=0.4550169600026051\n",
      "Current iteration=330, the loss=0.4399440122066609\n",
      "Current iteration=340, the loss=0.4382586035214138\n",
      "Current iteration=350, the loss=0.4289630620234629\n",
      "Current iteration=360, the loss=0.4209050148345616\n",
      "Current iteration=370, the loss=0.42796952091272106\n",
      "Current iteration=380, the loss=0.4288984811456762\n",
      "Current iteration=390, the loss=0.4280559509316297\n",
      "Current iteration=400, the loss=0.4247227887323558\n",
      "Current iteration=410, the loss=0.4361207492133911\n",
      "Current iteration=420, the loss=0.4221181003280757\n",
      "Current iteration=430, the loss=0.42066655186459434\n",
      "Current iteration=440, the loss=0.4194895632125487\n",
      "Current iteration=450, the loss=0.4309946731468699\n",
      "Current iteration=460, the loss=0.42636303409450466\n",
      "Current iteration=470, the loss=0.44083782058498383\n",
      "Current iteration=480, the loss=0.44005244361517226\n",
      "Current iteration=490, the loss=0.4158436520135687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/logistic.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=500, the loss=0.4120330370288551\n",
      "Current iteration=510, the loss=0.41696250401580937\n",
      "Current iteration=520, the loss=0.4132120374772826\n",
      "Current iteration=530, the loss=0.41564767191006746\n",
      "Current iteration=540, the loss=0.41846444541280275\n",
      "Current iteration=550, the loss=0.4146884351021209\n",
      "Current iteration=560, the loss=0.4188197729749331\n",
      "Current iteration=570, the loss=0.41935061758640224\n",
      "Current iteration=580, the loss=0.41132690491411683\n",
      "Current iteration=590, the loss=0.41355940095438404\n",
      "Current iteration=600, the loss=0.411977783477169\n",
      "Current iteration=610, the loss=0.4131298330525655\n",
      "Current iteration=620, the loss=0.4077640803759758\n",
      "Current iteration=630, the loss=0.4172416823593676\n",
      "Current iteration=640, the loss=0.3916309019607269\n",
      "Current iteration=650, the loss=0.4107203022312534\n",
      "Current iteration=660, the loss=0.4161810842997675\n",
      "Current iteration=670, the loss=0.41905723750343477\n",
      "Current iteration=680, the loss=0.42531431022273214\n",
      "Current iteration=690, the loss=0.5182887467873756\n",
      "Current iteration=700, the loss=0.4177855549628588\n",
      "Current iteration=710, the loss=0.4127985982392621\n",
      "Current iteration=720, the loss=0.4109551046783726\n",
      "Current iteration=730, the loss=0.4091261255592276\n",
      "Current iteration=740, the loss=0.4112643483523112\n",
      "The scaled loss=0.4142523351968968\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on half of train data\n",
    "y_binary = np.copy(y)\n",
    "y_binary[y_binary == -1] = 0\n",
    "X_log_tr, y_bin_tr, X_log_te, y_bin_te = split_data(tX, y_binary, 0.7) \n",
    "#w_log = logistic_regression_gradient_descent_demo(y_binary[:125000], tX[:125000], 0.0001, 750)\n",
    "w_log = logistic_regression_gradient_descent(y_bin_tr, X_log_tr, 0.005, 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.858619046299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/logistic.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 / (1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "vect = sigmoid(np.dot(X_log_te, w_log))\n",
    "vect[vect >= 0.5] = 1\n",
    "vect[vect < 0.5] = -1\n",
    "y_bin_te[y_bin_te == 0] = -1\n",
    "e_log = y_bin_te - vect[:,0]\n",
    "rmse_log = np.sqrt(2*calculate_mse(e_log))\n",
    "tot = 0\n",
    "print(\"RMSE: \", rmse_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/logistic.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 / (1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "# Prepare output if logistic regression\n",
    "pred_log = sigmoid(np.dot(tX_test.T, w_log))\n",
    "pred_log = pred_log[:,0]\n",
    "pred_log[pred_log >= 0.5] = 1\n",
    "pred_log[pred_log < 0.5] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[[ 0.69314718]]\n",
      "Current iteration=10, the loss=[[ 0.67295411]]\n",
      "Current iteration=20, the loss=[[ 0.65523154]]\n",
      "Current iteration=30, the loss=[[ 0.63744445]]\n",
      "Current iteration=40, the loss=[[ 0.63265065]]\n",
      "Current iteration=50, the loss=[[ 0.61098363]]\n",
      "Current iteration=60, the loss=[[ 0.59374678]]\n",
      "Current iteration=70, the loss=[[ 0.58235502]]\n",
      "Current iteration=80, the loss=[[ 0.57198742]]\n",
      "Current iteration=90, the loss=[[ 0.56067195]]\n",
      "Current iteration=100, the loss=[[ 0.54847404]]\n",
      "Current iteration=110, the loss=[[ 0.54105824]]\n",
      "Current iteration=120, the loss=[[ 0.53304697]]\n",
      "Current iteration=130, the loss=[[ 0.52897402]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/logistic.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=140, the loss=[[ 0.5205458]]\n",
      "Current iteration=150, the loss=[[ 0.52119361]]\n",
      "Current iteration=160, the loss=[[ 0.50715631]]\n",
      "Current iteration=170, the loss=[[ 0.49979092]]\n",
      "Current iteration=180, the loss=[[ 0.4884175]]\n",
      "Current iteration=190, the loss=[[ 0.49697983]]\n",
      "Current iteration=200, the loss=[[ 0.49690944]]\n",
      "Current iteration=210, the loss=[[ 0.47781862]]\n",
      "Current iteration=220, the loss=[[ 0.47303368]]\n",
      "Current iteration=230, the loss=[[ 0.48230667]]\n",
      "Current iteration=240, the loss=[[ 0.46985791]]\n",
      "Current iteration=250, the loss=[[ 0.46543297]]\n",
      "Current iteration=260, the loss=[[ 0.46142816]]\n",
      "Current iteration=270, the loss=[[ 0.45243041]]\n",
      "Current iteration=280, the loss=[[ 0.45527545]]\n",
      "Current iteration=290, the loss=[[ 0.4436634]]\n",
      "Current iteration=300, the loss=[[ 0.44499183]]\n",
      "Current iteration=310, the loss=[[ 0.44344566]]\n",
      "Current iteration=320, the loss=[[ 0.46020749]]\n",
      "Current iteration=330, the loss=[[ 0.4416031]]\n",
      "Current iteration=340, the loss=[[ 0.44234752]]\n",
      "Current iteration=350, the loss=[[ 0.43363525]]\n",
      "Current iteration=360, the loss=[[ 0.42451642]]\n",
      "Current iteration=370, the loss=[[ 0.43678647]]\n",
      "Current iteration=380, the loss=[[ 0.43337827]]\n",
      "Current iteration=390, the loss=[[ 0.43084479]]\n",
      "Current iteration=400, the loss=[[ 0.42802923]]\n",
      "Current iteration=410, the loss=[[ 0.43802123]]\n",
      "Current iteration=420, the loss=[[ 0.4244974]]\n",
      "Current iteration=430, the loss=[[ 0.42311629]]\n",
      "Current iteration=440, the loss=[[ 0.41973169]]\n",
      "Current iteration=450, the loss=[[ 0.43238612]]\n",
      "Current iteration=460, the loss=[[ 0.42883698]]\n",
      "Current iteration=470, the loss=[[ 0.44167242]]\n",
      "Current iteration=480, the loss=[[ 0.44388652]]\n",
      "Current iteration=490, the loss=[[ 0.42019864]]\n",
      "Current iteration=500, the loss=[[ 0.41627185]]\n",
      "Current iteration=510, the loss=[[ 0.42153186]]\n",
      "Current iteration=520, the loss=[[ 0.41309173]]\n",
      "Current iteration=530, the loss=[[ 0.41762486]]\n",
      "Current iteration=540, the loss=[[ 0.4223505]]\n",
      "Current iteration=550, the loss=[[ 0.41844755]]\n",
      "Current iteration=560, the loss=[[ 0.42323441]]\n",
      "Current iteration=570, the loss=[[ 0.42576897]]\n",
      "Current iteration=580, the loss=[[ 0.41617681]]\n",
      "Current iteration=590, the loss=[[ 0.4151239]]\n",
      "Current iteration=600, the loss=[[ 0.41543403]]\n",
      "Current iteration=610, the loss=[[ 0.42516728]]\n",
      "Current iteration=620, the loss=[[ 0.40983472]]\n",
      "Current iteration=630, the loss=[[ 0.42083676]]\n",
      "Current iteration=640, the loss=[[ 0.39345614]]\n",
      "Current iteration=650, the loss=[[ 0.4167358]]\n",
      "Current iteration=660, the loss=[[ 0.42044895]]\n",
      "Current iteration=670, the loss=[[ 0.41272519]]\n",
      "Current iteration=680, the loss=[[ 0.43127017]]\n",
      "Current iteration=690, the loss=[[ 0.48163232]]\n",
      "Current iteration=700, the loss=[[ 0.42385782]]\n",
      "Current iteration=710, the loss=[[ 0.41944781]]\n",
      "Current iteration=720, the loss=[[ 0.41620486]]\n",
      "Current iteration=730, the loss=[[ 0.40954283]]\n",
      "Current iteration=740, the loss=[[ 0.41237969]]\n",
      "Current iteration=750, the loss=[[ 0.4116259]]\n",
      "Current iteration=760, the loss=[[ 0.40685461]]\n",
      "Current iteration=770, the loss=[[ 0.40525193]]\n",
      "Current iteration=780, the loss=[[ 0.39944825]]\n",
      "Current iteration=790, the loss=[[ 0.41418102]]\n",
      "Current iteration=800, the loss=[[ 0.39918282]]\n",
      "Current iteration=810, the loss=[[ 0.41546045]]\n",
      "Current iteration=820, the loss=[[ 0.4946142]]\n",
      "Current iteration=830, the loss=[[ 0.4231306]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/logistic.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  l = np.log(1 + np.exp(np.dot(tx[n], w)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=840, the loss=[[ inf]]\n",
      "Current iteration=850, the loss=[[ 0.41119495]]\n",
      "Current iteration=860, the loss=[[ 0.40145252]]\n",
      "Current iteration=870, the loss=[[ 0.40737838]]\n",
      "Current iteration=880, the loss=[[ 0.44047662]]\n",
      "Current iteration=890, the loss=[[ 0.41915533]]\n",
      "The loss=0.4154495894015615\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on 70% of train data\n",
    "y_binary = np.copy(y)\n",
    "y_binary[y_binary == -1] = 0\n",
    "X_log_tr, y_bin_tr, X_log_te, y_bin_te = split_data(tX, y_binary, 0.7) \n",
    "#w_log = logistic_regression_gradient_descent_demo(y_binary[:125000], tX[:125000], 0.0001, 750)\n",
    "w_log_pen = logistic_regression_penalized_gradient_descent(y_bin_tr, X_log_tr, 0.005, 900, 10**(-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.85880537182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huguenin/ML_project1/projects/project1/scripts/Methods/logistic.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 / (1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "vect = sigmoid(np.dot(X_log_te, w_log_pen))\n",
    "vect[vect >= 0.5] = 1\n",
    "vect[vect < 0.5] = -1\n",
    "y_bin_te[y_bin_te == 0] = -1\n",
    "e_log = y_bin_te - vect[:,0]\n",
    "rmse_log = np.sqrt(2*calculate_mse(e_log))\n",
    "tot = 0\n",
    "print(\"RMSE: \", rmse_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'csv/sample-submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test.T)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'csv/sample-submission.csv'\n",
    "create_csv_submission(ids_test, pred_log, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
